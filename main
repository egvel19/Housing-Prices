import numpy as np 
import pandas as pd 
from sklearn.metrics import mean_squared_error
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor


#Load Data
train_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')
test_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')

# Outliers
rows_to_drop = [598, 955, 935, 1299, 250, 314, 336, 707, 379, 1183, 692, 186, 441, 186, 524, 739, 598, 955, 636, 1062, 1191, 496, 198, 1338]
train_df.drop(rows_to_drop, inplace=True)

# Feature engineering function
def engineer_features(df):
    df['houseage'] = df['YrSold'] - df['YearBuilt']
    df['houseremodelage'] = df['YrSold'] - df['YearRemodAdd']
    df['totalsf'] = df['1stFlrSF'] + df['2ndFlrSF'] + df['BsmtFinSF1'] + df['BsmtFinSF2']
    df['totalarea'] = df['GrLivArea'] + df['TotalBsmtSF']
    df['totalbaths'] = df['BsmtFullBath'] + df['FullBath'] + 0.5 * (df['BsmtHalfBath'] + df['HalfBath']) 
    df['totalporchsf'] = df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF']

# Feature engineering for test and training data
engineer_features(train_df)
engineer_features(test_df)

# prepare numerical and categorical data to process 
num_cols = train_df.select_dtypes(include=['int64', 'float64']).columns
num_cols = num_cols.drop('SalePrice')

cat_cols = train_df.select_dtypes(include=['object']).columns

# Define pipelines
num_pipeline = Pipeline(steps=[
    ('impute', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

cat_pipeline = Pipeline(steps=[
    ('impute', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# column transformer
col_trans = ColumnTransformer(transformers=[
    ('num_p', num_pipeline, num_cols),
    ('cat_p', cat_pipeline, cat_cols),
],
remainder='passthrough', 
n_jobs=-1)

# Prepare data, transform and split
X = train_df.drop('SalePrice', axis=1)
y = train_df['SalePrice']
X_preprocessed = col_trans.fit_transform(X)
X_preprocessed_test = col_trans.transform(test_df)

X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.4, random_state=0)

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
lr_mse = mean_squared_error(y_test, lr_pred)
print("Linear Regression MSE:", lr_mse)

#XGBoost
XGB = XGBRegressor(random_state=13)
param_grid_XGB = {
    'learning_rate': [0.05, 0.1, 0.2],
    'n_estimators': [300],
    'max_depth': [3],
    'min_child_weight': [1,2,3],
    'gamma': [0, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
}

xgb_cv = GridSearchCV(XGB, param_grid_XGB, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
xgb_cv.fit(X_train, y_train)
# np.sqrt(-1 * xgb_cv.best_score_)
best_xgb_model = xgb_cv.best_estimator_
xgb_pred = best_xgb_model.predict(X_test)
xgb_mse = mean_squared_error(y_test, xgb_pred)
print("XGBoost MSE:", xgb_mse)

# Predict for test data and prepare submission
y_test_pred = best_xgb_model.predict(X_preprocessed_test)
submission_df = pd.DataFrame({
    'Id': test_df['Id'],
    'SalePrice': y_test_pred
})

submission_df.to_csv('submission.csv', index = False)
